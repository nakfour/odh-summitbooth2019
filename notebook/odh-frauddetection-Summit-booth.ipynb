{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Credit Card Data to ODH Ceph-Nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'tx00000000000000000001c-005cae553f-101a-default',\n",
       "  'HostId': '',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'transfer-encoding': 'chunked',\n",
       "   'x-amz-request-id': 'tx00000000000000000001c-005cae553f-101a-default',\n",
       "   'content-type': 'application/xml',\n",
       "   'date': 'Wed, 10 Apr 2019 20:42:39 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'IsTruncated': False,\n",
       " 'Marker': '',\n",
       " 'Contents': [{'Key': 'uploaded/creditcard-sample10k.csv',\n",
       "   'LastModified': datetime.datetime(2019, 4, 10, 20, 42, 39, 778000, tzinfo=tzlocal()),\n",
       "   'ETag': '\"4823930c39124522f6c5e308de24128e\"',\n",
       "   'Size': 5467621,\n",
       "   'StorageClass': 'STANDARD',\n",
       "   'Owner': {'DisplayName': 'Ceph demo user', 'ID': 'nano'}}],\n",
       " 'Name': 'OPEN',\n",
       " 'Prefix': '',\n",
       " 'MaxKeys': 1000,\n",
       " 'EncodingType': 'url'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "# Create an S3 client\n",
    "s3 = boto3.client(service_name='s3',aws_access_key_id='foo', aws_secret_access_key='bar', endpoint_url='http://ceph:8000')\n",
    "s3.create_bucket(Bucket='OPEN')\n",
    "s3.list_buckets()\n",
    "\n",
    "key = \"uploaded/creditcard-sample10k.csv\"\n",
    "s3.upload_file(Bucket='OPEN', Key=key, Filename=\"creditcard-sample10k.csv\")\n",
    "s3.list_objects(Bucket='OPEN')\n",
    "#s3.list_buckets()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data Using Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting\n",
      "{'ResponseMetadata': {'RequestId': 'tx00000000000000000001d-005cae5543-101a-default', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'transfer-encoding': 'chunked', 'x-amz-request-id': 'tx00000000000000000001d-005cae5543-101a-default', 'content-type': 'application/xml', 'date': 'Wed, 10 Apr 2019 20:42:43 GMT'}, 'RetryAttempts': 0}, 'IsTruncated': False, 'Marker': '', 'Contents': [{'Key': 'uploaded/creditcard-sample10k.csv', 'LastModified': datetime.datetime(2019, 4, 10, 20, 42, 39, 778000, tzinfo=tzlocal()), 'ETag': '\"4823930c39124522f6c5e308de24128e\"', 'Size': 5467621, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'Ceph demo user', 'ID': 'nano'}}], 'Name': 'OPEN', 'Prefix': '', 'MaxKeys': 1000, 'EncodingType': 'url'}\n",
      "Start\n",
      "Total number of rows: 10000\n",
      "Total number of rows with fraud\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import boto3\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve\n",
    "        \n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client(service_name='s3',aws_access_key_id='foo', aws_secret_access_key='bar', endpoint_url='http://ceph:8000')\n",
    "\n",
    "print(\"getting\")\n",
    "print(s3.list_objects(Bucket='OPEN'))\n",
    "\n",
    "\n",
    "print(\"Start\")\n",
    "#on openshift\n",
    "############## spark\n",
    "spark = SparkSession.builder.appName(\"odh-pyspark\").master('spark://' + os.environ['SPARK_CLUSTER'] + ':7077').getOrCreate()\n",
    "\n",
    "\n",
    "#Set the Hadoop configurations to access Ceph S3\n",
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "\n",
    "#hadoopConf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "#hadoopConf.set(\"fs.s3.awsAccessKeyId\", 'foo')\n",
    "#hadoopConf.set(\"fs.s3.awsSecretAccessKey\", 'bar')\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\");\n",
    "hadoopConf.set(\"fs.s3a.access.key\", 'foo') \n",
    "hadoopConf.set(\"fs.s3a.secret.key\", 'bar')\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", 'http://ceph:8000') \n",
    "#hadoopConf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\" )\n",
    "\n",
    "\n",
    "#df = spark.read.csv(\"s3a://OPEN/uploaded/creditcard-sample10k.csv\",header=True)\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"True\").option(\"mode\", \"DROPMALFORMED\").load(\"s3a://OPEN/uploaded/creditcard-sample10k.csv\")\n",
    "#df.show()\n",
    "print(\"Total number of rows: %d\" % df.count())\n",
    "\n",
    "### Get number of rowns with fraud is detected\n",
    "print(\"Total number of rows with fraud\")\n",
    "print(df[(df['Class']==1)].count())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sklearn Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Number of train examples: %s 6977\n",
      "Number of test  examples: %s 3023\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "#df.show()\n",
    "#sort dataframe by Time\n",
    "#df.sort_values(by='Time', inplace=True)\n",
    "df.orderBy(\"Time\")\n",
    "\n",
    "\n",
    "#number of rows in the dataset\n",
    "n_samples = df.count()\n",
    "print(n_samples)\n",
    "\n",
    "\n",
    "#Split into train and test\n",
    "train_size = 0.75\n",
    "\n",
    "#n_train = int(train_size * n_samples)\n",
    "\n",
    "#df_train = df.iloc[0:n_train] #first n_train rows are train\n",
    "#df_test = df.iloc[n_train:] #rest is test\n",
    "\n",
    "(df_train, df_test) = df.randomSplit([.7, .3])\n",
    "print('Number of train examples: %s', df_train.count())\n",
    "print('Number of test  examples: %s', df_test.count())\n",
    "\n",
    "#Define features and target variables for convenience.\n",
    "drop_time_class = ['Time', 'Class']\n",
    "drop_class=['Class']\n",
    "\n",
    "\n",
    "features_train = df_train.drop(*drop_time_class)\n",
    "target_train = df_train.select(\"Class\")\n",
    "\n",
    "features_test = df_test.drop(*drop_time_class)\n",
    "target_test = df_test.select(\"Class\")\n",
    "features_test.printSchema()\n",
    "\n",
    "\n",
    "#model = RandomForestClassifier(n_estimators=100, \n",
    "#                               max_depth=4, \n",
    "#                               n_jobs=10)\n",
    "\n",
    "#model = RandomForestRegressor(n_estimators=100, \n",
    " #                             max_depth=4, \n",
    "  #                             n_jobs=10)\n",
    "\n",
    "model = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=100)\n",
    "\n",
    "                               \n",
    "#Convert to pandas\n",
    "features_train_pd = features_train.toPandas()\n",
    "target_train_pd = target_train.toPandas()\n",
    "\n",
    "model.fit(features_train_pd, target_train_pd.values.ravel())\n",
    "\n",
    "#predict probability of sample being in class 1\n",
    "#pred_train = model.predict_proba(features_train.toPandas())\n",
    "#pred_test = model.predict_proba(features_test.toPandas())\n",
    "\n",
    "pred_train = model.predict(features_train.toPandas())\n",
    "pred_test = model.predict(features_test.toPandas())\n",
    "\n",
    "  \n",
    "#save mode in filesystem\n",
    "joblib.dump(model, 'model.pkl') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30525389]\n"
     ]
    }
   ],
   "source": [
    "# Create the pandas DataFrame \n",
    "\n",
    "data = [[0,-1.3598071336738,-0.0727811733098497,2.53634673796914,1.37815522427443,-0.33832076994251803,0.462387777762292,0.239598554061257,0.0986979012610507,0.363786969611213,0.0907941719789316,-0.551599533260813,-0.617800855762348,-0.991389847235408,-0.31116935369987897,1.46817697209427,-0.47040052525947795,0.20797124192924202,0.0257905801985591,0.403992960255733,0.251412098239705,-0.018306777944153,0.277837575558899,-0.110473910188767,0.0669280749146731,0.12853935827352803,-0.189114843888824,0.13355837674038698,-0.0210530534538215,149.62]]\n",
    "data_fraud=[[0.0,-2.3122265423262998,1.9519920106415802,-1.6098507322976898,3.9979055875468,-0.522187864667764,-1.4265453192059498,-2.5373873062457903,1.39165724829804,-2.77008927719433,-2.7722721446591496,3.20203320709635,-2.89990738849473,-0.595221881324605,-4.28925378244217,0.38972412027448705,-1.14074717980657,-2.83005567450437,-0.0168224681808257,0.4169557050379071,0.126910559061474,0.517232370861764,-0.0350493686052974,-0.46521107618238794,0.320198198514526,0.0445191674731724,0.177839798284401,0.26114500256767703,-0.143275874698919,0]]\n",
    "test= pd.DataFrame(data_fraud, columns = ['Time','V1','V2','V3','V4', 'V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18',\t'V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','Amount'])\n",
    "test.head()\n",
    "\n",
    "print(model.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.48583181e-05]\n",
      "[7.48583181e-05]\n",
      "[0.0097214]\n",
      "[7.48583181e-05]\n",
      "[7.48583181e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-5d97dff98241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#print(rowdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrowdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Sending one example\n",
    "#jsonfinalStripped = \"0.0,-1.3598071337,-0.0727811733,2.536346738,1.3781552243,-0.3383207699,0.4623877778,0.2395985541,0.0986979013,0.3637869696,0.090794172,-0.5515995333,-0.6178008558,-0.9913898472,-0.3111693537,1.4681769721,-0.4704005253,0.2079712419,0.0257905802,0.4039929603,0.2514120982,-0.0183067779,0.2778375756,-0.1104739102,0.0669280749,0.1285393583,-0.1891148439,0.1335583767,-0.0210530535,149.62\"\n",
    "#y=[float(i) for i in jsonfinalStripped.split(',')]\n",
    "#print(y)\n",
    "#rowdf = pd.DataFrame([y], columns = ['Time','V1','V2','V3','V4', 'V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','Amount'])\n",
    "#print(rowdf)\n",
    "#print(model.predict(rowdf))\n",
    "\n",
    "#### Sending all rows from test dataframe\n",
    "#### select fraud transactions from the test dataframe\n",
    "#fraudTest = df_test.toPandas()['Class']== 1\n",
    "fraudTest = df_test.toPandas().loc[df_test.toPandas()['Class']== 1]\n",
    "notFraudTest = df_test.toPandas().loc[df_test.toPandas()['Class']== 0]\n",
    "#print(fraudTest)\n",
    "#print(fraudTest.shape[0])\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Class', '_c0'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Class', '_c0'])\n",
    "#print(fraudTestFeatures)\n",
    "#Drop the class column\n",
    "\n",
    "#Send all the fraud tests\n",
    "#for index, row in features_test.toPandas().iterrows():\n",
    "for index, row in notFraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    #print(data.tolist())\n",
    "    rowdf = pd.DataFrame([data.tolist()], columns = ['Time','V1','V2','V3','V4', 'V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','Amount'])\n",
    "    #print(rowdf)\n",
    "    print(model.predict(rowdf))\n",
    "    time.sleep(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model to Ceph Nano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'tx0000000000000000000ac-005cae0801-101a-default',\n",
       "  'HostId': '',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'transfer-encoding': 'chunked',\n",
       "   'x-amz-request-id': 'tx0000000000000000000ac-005cae0801-101a-default',\n",
       "   'content-type': 'application/xml',\n",
       "   'date': 'Wed, 10 Apr 2019 15:13:05 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'IsTruncated': False,\n",
       " 'Marker': '',\n",
       " 'Contents': [{'Key': 'uploaded/model.pkl',\n",
       "   'LastModified': datetime.datetime(2019, 4, 10, 15, 13, 5, 29000, tzinfo=tzlocal()),\n",
       "   'ETag': '\"b88dd8dee1e5358c9fd1db76aa5a3d14\"',\n",
       "   'Size': 64626,\n",
       "   'StorageClass': 'STANDARD',\n",
       "   'Owner': {'DisplayName': 'Ceph demo user', 'ID': 'nano'}}],\n",
       " 'Name': 'MODEL',\n",
       " 'Prefix': '',\n",
       " 'MaxKeys': 1000,\n",
       " 'EncodingType': 'url'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "# Create an S3 client\n",
    "s3 = boto3.client(service_name='s3',aws_access_key_id='foo', aws_secret_access_key='bar', endpoint_url='http://ceph:8000')\n",
    "s3.create_bucket(Bucket='MODEL')\n",
    "s3.list_buckets()\n",
    "\n",
    "key = \"uploaded/model.pkl\"\n",
    "s3.upload_file(Bucket='MODEL', Key=key, Filename=\"model.pkl\")\n",
    "s3.list_objects(Bucket='MODEL')\n",
    "#s3.list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Test Downloading Model\n",
    "s3 = boto3.client(service_name='s3',aws_access_key_id='foo', aws_secret_access_key='bar', endpoint_url='http://ceph:8000')\n",
    "key = \"uploaded/model.pkl\"\n",
    "print(\"Trying to download model\")\n",
    "s3.list_objects(Bucket='MODEL')\n",
    "key = \"uploaded/model.pkl\"\n",
    "s3.download_file(Bucket='MODEL', Key=key, Filename=\"model2.pkl\")\n",
    "#s3.download_file(Bucket='MODEL', Key=key, Filename=\"model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install OpenShift client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -o oc.tar.gz -L https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz\n",
    "tar xzf oc.tar.gz\n",
    "cp openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit/oc ~/../bin/oc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login into Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged into \"https://sde-ci-works05.3a2m.lab.eng.bos.redhat.com:8443\" as \"jnakfour\" using the token provided.\n",
      "\n",
      "You have access to the following projects and can switch between them with 'oc project <projectname>':\n",
      "\n",
      "    argotesting\n",
      "    cronjob-prune-resources\n",
      "    default\n",
      "    kube-public\n",
      "    kube-service-catalog\n",
      "    kube-system\n",
      "    kubeflow\n",
      "    management-infra\n",
      "  * opendatahub\n",
      "    openshift\n",
      "    openshift-ansible-service-broker\n",
      "    openshift-console\n",
      "    openshift-infra\n",
      "    openshift-logging\n",
      "    openshift-monitoring\n",
      "    openshift-node\n",
      "    openshift-sdn\n",
      "    openshift-template-service-broker\n",
      "    openshift-web-console\n",
      "    panbalag\n",
      "    runningcronjob\n",
      "    sparktesting\n",
      "    tiller\n",
      "\n",
      "Using project \"opendatahub\".\n",
      "Already on project \"opendatahub\" on server \"https://sde-ci-works05.3a2m.lab.eng.bos.redhat.com:8443\".\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc login https://sde-ci-works05.3a2m.lab.eng.bos.redhat.com:8443 --token=REIgyPhIBWipB9qJavJCrZNTqsvxrTUmT8kAWljadU4\n",
    "oc project opendatahub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Model With Seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on project \"opendatahub\" on server \"https://sde-ci-works05.3a2m.lab.eng.bos.redhat.com:8443\".\n",
      "seldondeployment.machinelearning.seldon.io/mymodel created\n",
      "NAME      AGE\n",
      "mymodel   0s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "oc project opendatahub\n",
    "oc create -n opendatahub -f https://raw.githubusercontent.com/nakfour/odh-kubeflow/master/mymodel.json\n",
    "oc get seldondeployments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Model Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc9e7a37-505e-455c-84b1-65436feb5943\n",
      "{\n",
      "  \"meta\": {\n",
      "    \"puid\": \"j7fj3mopfaop9so2aiaojs9la7\",\n",
      "    \"tags\": {\n",
      "    },\n",
      "    \"routing\": {\n",
      "    },\n",
      "    \"requestPath\": {\n",
      "      \"mymodel\": \"nakfour/mymodel\"\n",
      "    },\n",
      "    \"metrics\": []\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"names\": [],\n",
      "    \"ndarray\": [3.7328284527878166E-5]\n",
      "  }\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   146    0   117  100    29   3297    817 --:--:-- --:--:-- --:--:--  3342\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* About to connect() to seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io port 80 (#0)\n",
      "*   Trying 10.16.208.2...\n",
      "* Connected to seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io (10.16.208.2) port 80 (#0)\n",
      "> POST /api/v0.1/predictions HTTP/1.1\r\n",
      "> User-Agent: curl/7.29.0\r\n",
      "> Host: seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io\r\n",
      "> Accept: */*\r\n",
      "> Authorization: Bearer  fc9e7a37-505e-455c-84b1-65436feb5943\r\n",
      "> Content-Type: application/json\r\n",
      "> Content-Length: 399\r\n",
      "> \r\n",
      "} [data not shown]\n",
      "* upload completely sent off: 399 out of 399 bytes\n",
      "< HTTP/1.1 200 \r\n",
      "< Access-Control-Allow-Origin: *\r\n",
      "< Access-Control-Allow-Methods: POST, GET, OPTIONS, DELETE\r\n",
      "< Access-Control-Max-Age: 3600\r\n",
      "< Access-Control-Allow-Headers: API-Key, accept, Content-Type, x-requested-with, authorization\r\n",
      "< X-Application-Context: application\r\n",
      "< X-Content-Type-Options: nosniff\r\n",
      "< X-XSS-Protection: 1; mode=block\r\n",
      "< Cache-Control: no-cache, no-store, max-age=0, must-revalidate\r\n",
      "< Pragma: no-cache\r\n",
      "< Expires: 0\r\n",
      "< X-Frame-Options: DENY\r\n",
      "< Content-Type: application/json\r\n",
      "< Content-Length: 260\r\n",
      "< Date: Wed, 10 Apr 2019 15:15:51 GMT\r\n",
      "< Set-Cookie: b22a9bc811641895de502f74dce0996a=dde728ee0dbc8a9bf520f2fc49b8ad45; path=/; HttpOnly\r\n",
      "< \r\n",
      "{ [data not shown]\n",
      "\r",
      "100   659  100   260  100   399    935   1435 --:--:-- --:--:-- --:--:--  1435\r",
      "100   659  100   260  100   399    934   1434 --:--:-- --:--:-- --:--:--  1430\n",
      "* Connection #0 to host seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io left intact\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cp jq ~/../bin/jq\n",
    "chmod 777 ~/../bin/jq\n",
    "export TOKENJSON=$(curl -XPOST -u oauth-key:oauth-secret http://seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io/oauth/token -d 'grant_type=client_credentials')\n",
    "export TOKEN=$(echo $TOKENJSON | jq \".access_token\" -r)\n",
    "echo $TOKEN\n",
    "#curl -v --header \"Authorization: Bearer  $TOKEN\" http://seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io/api/v0.1/predictions -d '{\"strData\": [\"test\":\"202\"]}' -H \"Content-Type: application/json\"\n",
    "#curl -v --header \"Authorization: Bearer  $TOKEN\" http://seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io/api/v0.1/predictions -d '{\"strData\": {\"Time\":0.0,\"V1\":-1.3598071337,\"V2\":-0.0727811733,\"V3\":2.536346738,\"V4\":1.3781552243,\"V5\":-0.3383207699,\"V6\":0.4623877778,\"V7\":0.2395985541,\"V8\":0.0986979013,\"V9\":0.3637869696,\"V10\":0.090794172,\"V11\":-0.5515995333,\"V12\":-0.6178008558,\"V13\":-0.9913898472,\"V14\":-0.3111693537,\"V15\":1.4681769721,\"V16\":-0.4704005253,\"V17\":0.2079712419,\"V18\":0.0257905802,\"V19\":0.4039929603,\"V20\":0.2514120982,\"V21\":-0.0183067779,\"V22\":0.2778375756,\"V23\":-0.1104739102,\"V24\":0.0669280749,\"V25\":0.1285393583,\"V26\":-0.1891148439,\"V27\":0.1335583767,\"V28\":-0.0210530535,\"Amount\":149.62}}' -H \"Content-Type: application/json\"\n",
    "#'{\"strData\": {\"_c0\":0.0,\"V1\":-1.3598071337,\"V2\":-0.0727811733,\"V3\":2.536346738,\"V4\":1.3781552243,\"V5\":-0.3383207699,\"V6\":0.4623877778,\"V7\":0.2395985541,\"V8\":0.0986979013,\"V9\":0.3637869696,\"V10\":0.090794172,\"V11\":-0.5515995333,\"V12\":-0.6178008558,\"V13\":-0.9913898472,\"V14\":-0.3111693537,\"V15\":1.4681769721,\"V16\":-0.4704005253,\"V17\":0.2079712419,\"V18\":0.0257905802,\"V19\":0.4039929603,\"V20\":0.2514120982,\"V21\":-0.0183067779,\"V22\":0.2778375756,\"V23\":-0.1104739102,\"V24\":0.0669280749,\"V25\":0.1285393583,\"V26\":-0.1891148439,\"V27\":0.1335583767,\"V28\":-0.0210530535,\"Amount\":149.62}}'\n",
    "curl -v --header \"Authorization: Bearer  $TOKEN\" http://seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io/api/v0.1/predictions -d '{\"strData\": \"0.0,-1.3598071337,-0.0727811733,2.536346738,1.3781552243,-0.3383207699,0.4623877778,0.2395985541,0.0986979013,0.3637869696,0.090794172,-0.5515995333,-0.6178008558,-0.9913898472,-0.3111693537,1.4681769721,-0.4704005253,0.2079712419,0.0257905802,0.4039929603,0.2514120982,-0.0183067779,0.2778375756,-0.1104739102,0.0669280749,0.1285393583,-0.1891148439,0.1335583767,-0.0210530535,149.62\"}' -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Model Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc9e7a37-505e-455c-84b1-65436feb5943\n",
      "1.0,1.1918571113148602,0.26615071205963,0.16648011335321,0.448154078460911,0.0600176492822243,-0.0823608088155687,-0.0788029833323113,0.0851016549148104,-0.255425128109186,-0.16697441400461402,1.6127266610547901,1.06523531137287,0.48909501589608,-0.143772296441519,0.635558093258208,0.463917041022171,-0.114804663102346,-0.18336127012399397,-0.14578304132525902,-0.0690831352230203,-0.225775248033138,-0.6386719527718511,0.10128802125323402,-0.33984647552912706,0.167170404418143,0.125894532368176,-0.00898309914322813,0.0147241691924927,2.69\n",
      "3.7328284527878166e-05\n",
      "13.0,1.0693735878819002,0.287722129331455,0.8286127266342809,2.71252042961718,-0.178398016248009,0.33754373028296797,-0.0967168617395962,0.11598173554659698,-0.22108256623619396,0.4602304443016779,-0.7736569305266892,0.32338724546722,-0.0110758870883779,-0.17848517517791604,-0.65556427824926,-0.19992517131173,0.1240054151819,-0.980496201537345,-0.982916082135047,-0.153197231044512,-0.0368755317335273,0.0744124028162195,-0.0714074332998586,0.104743752596029,0.548264725394119,0.10409415316278098,0.0214910583643189,0.021293311477486,27.5\n",
      "3.7328284527878166e-05\n",
      "15.0,-0.7524170429566049,0.345485415344747,2.05732291276727,-1.4686432984004598,-1.1583936804082,-0.0778498291166733,-0.608581418236123,0.00360348436201849,-0.436166983515744,0.7477308271928019,-0.7939806028372208,-0.770406728847129,1.04762699748088,-1.06660368148653,1.1069534566214099,1.66011355713381,-0.279265373246772,-0.419994141181313,0.432535348618175,0.263450864446125,0.499624954671111,1.3536504855723102,-0.256573280448308,-0.0650837078816517,-0.0391243535426488,-0.0870864732146962,-0.18099750009272103,0.129394059390202,15.99\n",
      "3.7328284527878166e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-3ffef2cd331d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mpredictionArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatafield\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ndarray'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionArray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Testing the served model from python using the test dataframe\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Get the token\n",
    "post_data = {\"grant_type\": \"client_credentials\"}\n",
    "requestOauth = requests.post('http://seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io/oauth/token', auth=('oauth-key', 'oauth-secret'), data=post_data, json={'grant_type=client_credentials'})\n",
    "#print(r.status_code)\n",
    "#print(requestOauth.content)\n",
    "data = requestOauth.json();\n",
    "print(data['access_token'])\n",
    "access_token = data['access_token']\n",
    "\n",
    "headers = {'Content-type': 'application/json', 'Authorization': 'Bearer {}'.format(access_token)}\n",
    "#Read the test dataframe and stream each row\n",
    "for index, row in features_test.toPandas().iterrows():\n",
    "    data = row\n",
    "    #print(data.tolist())\n",
    "    str1 = ','.join(str(e) for e in  data)\n",
    "    print(str1)\n",
    "    # Send the post request for the prediction\n",
    "    #requestPrediction = requests.post('http://seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io/api/v0.1/predictions', headers=headers, json={\"strData\": \"0.0,-1.3598071337,-0.0727811733,2.536346738,1.3781552243,-0.3383207699,0.4623877778,0.2395985541,0.0986979013,0.3637869696,0.090794172,-0.5515995333,-0.6178008558,-0.9913898472,-0.3111693537,1.4681769721,-0.4704005253,0.2079712419,0.0257905802,0.4039929603,0.2514120982,-0.0183067779,0.2778375756,-0.1104739102,0.0669280749,0.1285393583,-0.1891148439,0.1335583767,-0.0210530535,149.62\"})\n",
    "    requestPrediction = requests.post('http://seldon-core-seldon-apiserver-opendatahub.10.16.208.2.nip.io/api/v0.1/predictions', headers=headers, json={\"strData\": str1 })\n",
    "    \n",
    "    #print(requestPrediction.status_code)\n",
    "    #print(requestPrediction.content)\n",
    "    predictionData = requestPrediction.json();\n",
    "    datafield = predictionData['data']\n",
    "    predictionArray = datafield['ndarray']\n",
    "    print(predictionArray[0])\n",
    "    time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress bar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "import subprocess\n",
    "\n",
    "from IPython.display import clear_output\n",
    "def update_progress(progress, status):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress*100)\n",
    "    print(text + \" \" + status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tfjob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc create -f https://raw.githubusercontent.com/nakfour/odh-kubeflow/master/tfjob-mnst.yaml\n",
    "#oc get tfjobs\n",
    "oc get -o yaml tfjobs dist-mnist-for-e2e-test\n",
    "#oc delete tfjob dist-mnist-for-e2e-test\n",
    "#oc get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfJob Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress=0\n",
    "while progress < 1:\n",
    "    output = subprocess.check_output(\"oc get -o yaml tfjobs dist-mnist-for-e2e-test\", shell=True)\n",
    "    for row in output.decode(\"utf-8\").split('\\n'):\n",
    "        if ': ' in row:\n",
    "            key, value = row.split(': ')\n",
    "            if((value=='TFJobCreated') and (progress< 0.33)):\n",
    "                #print(\"PyTorchJobCreated\")\n",
    "                progress=0.33\n",
    "                update_progress(progress, 'TFJobCreated')\n",
    "            if((value=='TFJobRunning') and (progress< 0.66)):\n",
    "                #print(\"PyTorchJobRunning\")\n",
    "                progress=0.66\n",
    "                update_progress(progress,'TFJobRunning')\n",
    "            if((value == 'TFJobSucceeded') and (progress< 1)):\n",
    "                #print(\"PyTorchJobSucceeded\")\n",
    "                progress=1\n",
    "                update_progress(1,'TFJobSucceeded')\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pytorch learning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#oc create -f https://raw.githubusercontent.com/nakfour/odh-kubeflow/master/pvc-kubeflow-gcfs.yaml\n",
    "oc create -f https://raw.githubusercontent.com/nakfour/odh-kubeflow/master/pytorch_mnist_DDP_CPU.yaml\n",
    "oc get pytorchjob\n",
    "#oc get -o yaml pytorchjobs pytorch-mnist-ddp-cpu \n",
    "oc get pods -l pytorch_job_name=pytorch-mnist-ddp-cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress=0\n",
    "while progress < 1:\n",
    "    output = subprocess.check_output(\"oc get -o yaml pytorchjobs pytorch-mnist-ddp-cpu\", shell=True)\n",
    "    for row in output.decode(\"utf-8\").split('\\n'):\n",
    "        if ': ' in row:\n",
    "            key, value = row.split(': ')\n",
    "            if((value=='PyTorchJobCreated') and (progress< 0.33)):\n",
    "                #print(\"PyTorchJobCreated\")\n",
    "                progress=0.33\n",
    "                update_progress(progress, 'PyTorchJobCreated')\n",
    "            if((value=='PyTorchJobRunning') and (progress< 0.66)):\n",
    "                #print(\"PyTorchJobRunning\")\n",
    "                progress=0.66\n",
    "                update_progress(progress,'PyTorchJobRunning')\n",
    "            if((value == 'PyTorchJobSucceeded') and (progress< 1)):\n",
    "                #print(\"PyTorchJobSucceeded\")\n",
    "                progress=1\n",
    "                update_progress(1,'PyTorchJobSucceeded')\n",
    "    time.sleep(5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
